ğŸ“˜ RAG Personal Assistant (FastAPI + ChromaDB)

A lightweight **Retrieval-Augmented Generation (RAG)** system that lets users upload PDFs, index their content into embeddings, and ask natural-language questions. The system retrieves the most relevant chunks using **ChromaDB** and responds using the stored text data.

---

ğŸš€ Features

* ğŸ“„ Upload any PDF file
* ğŸ” Automatic text extraction
* âœ‚ï¸ Text chunking with custom splitter
* ğŸ§  Embedding generation using **Sentence-Transformer (MiniLM-L6-v2)**
* ğŸ“¦ Vector storage in **ChromaDB**
* ğŸ¤– Ask questions and retrieve relevant document context
* ğŸ–¥ï¸ Simple HTML/JS frontend UI
* âš¡ FastAPI backend with REST APIs

---

ğŸ“‚ Project Structure

```
rag-personal-assistant/
â”‚
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ pdf_loader.py
â”‚   â”‚   â”œâ”€â”€ text_splitter.py
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ embeddings.py
â”‚   â”‚   â”œâ”€â”€ vector_store.py
â”‚   â”œâ”€â”€ venv/
â”‚
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ index.html
â”‚   â”œâ”€â”€ styles.css
â”‚   â”œâ”€â”€ script.js
â”‚
â””â”€â”€ README.md
```

---

ğŸ”§ Technologies Used

**Backend**

* FastAPI
* Python 3.10+
* sentence-transformers
* ChromaDB
* PyPDF2

**Frontend**

* HTML
* CSS
* JavaScript (vanilla fetch API)

---

â–¶ï¸ How to Run the Backend

1. Open terminal
2. Navigate to backend folder

```
cd backend
```

3. Install dependencies

```
pip install -r requirements.txt
```

4. Start FastAPI server

```
uvicorn main:app --reload
```

Backend runs at:

```
http://127.0.0.1:8000
```

Swagger UI:

```
http://127.0.0.1:8000/docs
```

---

ğŸ’» How to Run the Frontend

1. Open `frontend/index.html` in any browser
2. Upload a PDF
3. Ask questions in the chat box

---

ğŸ“Œ API Endpoints

**POST /upload**

Uploads & indexes a PDF.

**GET /ask?q=your question**

Retrieves the best matching chunk.

---

ğŸ§  How It Works (RAG Pipeline)

1. **Upload PDF**
2. **Extract text using PyPDF2**
3. **Split text into chunks (500 chars)**
4. **Embed chunks using MiniLM-L6-v2**
5. **Store embeddings in ChromaDB**
6. **Query vector DB with user question**
7. **Return top relevant answer**

---

ğŸ§ª Example Output

**User:** What is the aim of this module?
**Assistant:** *Extracted from the most relevant PDF chunkâ€¦*

---

ğŸ¤ Contributing

Pull requests are welcome!
If you'd like to add OCR, multi-file support, or LLM integration, feel free to contribute.

---

ğŸ“„ License

MIT License.

---
